@article{DESOLIVEIRA2023622,
title = {FastAiAlloc: A real-time multi-resources allocation framework proposal based on predictive model and multiple optimization strategies},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {622-636},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003126},
author = {Marcos {de S. Oliveira} and Francisco Erivaldo Fernandes and Lukas Cerveny and Flávia Akemi Miyazaki and Leonardo Valeriano Neri and Alan {da Silva} and Beatriz Leandro Bonafini and Victor Medeiros Outtes Alves and Órion Darshan Winter {de Lima}},
keywords = {Cloud computing, Resource allocation, Self-Organizing Maps, Exact programming, Particle Swarm Optimization, Genetic Algorithms},
abstract = {In cloud platforms, a common task is to allocate computational resources, e.g., memory and CPU, requested by applications and users. The allocation of these resources, which is an optimal load-balancing task, is considered an NP-Hard problem, being a challenging research area. There are many works proposed in the literature to address this problem. They use several strategies to deal with this optimization problem such as evolutionary algorithms, exact programming and also heuristics. However, some steps of the allocation process are not considered by these works, which are sometimes treated separately and not in an integrated manner. These steps include the applications and users resource consumption request profile as part of the optimization process and defining adequate metrics to check the optimized allocation. To integrate these steps, this work proposes a framework based on the following strategies, widely used in the literature: Genetic Algorithms (GA), Particle Swarm Optimization (PSO) and Linear Programming, besides our Heuristic approach. Furthermore, we restricted the resource allocation optimization to a real-time scenario, facilitating its use in an industrial process. In addition, Key Performance Indicators (KPIs) are proposed to carry out a comparative study in a public dataset. Our experiments showed that our proposed framework achieved better results than the baseline one, e.g., using a random allocation. In some cases, we observed an increase of almost 60% in performance compared with the baseline. In addition, when trying to balance memory and CPU consumption in the cluster, the linear approach performed the best, while the GA achieved the best result in allocating different user profiles across the cluster.}
}

@article{OLIVEIRA2022118092,
title = {Unsupervised feature selection method based on iterative similarity graph factorization and clustering by modularity},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118092},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118092},
url = {https://www.sciencedirect.com/science/article/pii/S095741742201288X},
author = {Marcos de S. Oliveira and Sergio R. de M. Queiroz and Francisco de A.T. de Carvalho},
keywords = {Feature selection, Unsupervised learning, Low-rank approximation, Graph modularity},
abstract = {Feature selection is an important research area aimed at eliminating unwanted features from high-dimensional datasets. Feature selection methods are categorized according to the availability of labels. Supervised methods usually select features that have high correlations with the data labels, however, this cannot be done by unsupervised methods, since the labels are not available, making the development of them even more challenging. Some unsupervised methods proposed in the literature get around this problem by generating “pseudo-labels”, using clustering techniques, and then making feature selection as in the supervised scenario. There are also methods in which the selection is not guided by a clustering criterion, generally performing some kind of reconstruction of the original dataset in low dimensionality. However, in both approaches a local structure is generated, usually starting from a similarity graph built by using the entire set of features. This may compromise the results of the methods when there are many irrelevant and noisy features, which makes it difficult to reveal patterns or an initial representation of the data. Another drawback of some current methods is the large amount of hyper-parameters, or the computational time required for a single execution, which can render such methods unfeasible for some large datasets. In order to address these problems, it is proposed in this work a new unsupervised feature selection method, called KNMFS, which performs the scoring of the features in low computational time using a three-step procedure: (1) a similarity graph learning step based on non-negative iterative matrix factorization together with a Gaussian filter to mitigate the noise effects caused by irrelevant features; (2) a clustering step from the learned graph using a modularity optimization strategy and (3) a random forest algorithm is applied to compute the scores based on the labels generated by the clustering step. To verify the effectiveness of the proposed method, experiments in real and synthetic datasets were conducted. In both cases, the obtained results showed that the KNMFS method, compared to other state-of-the-art methods, obtained good results according to the metrics of Accuracy, ARI and NMI. Friedman’s statistical tests were also performed to give stronger evidence to the reported results.}
}

@article{de2020unsupervised,
  title={Unsupervised feature selection methodology for clustering in high dimensionality datasets},
  author={de Souza Oliveira, Marcos and Queiroz, Sergio},
  journal={RITA},
  volume={27},
  number={2},
  pages={30-41},
  year={2020},
  doi={http://dx.doi.org/10.22456/2175-2745.96081},
  keywords={Feature Selection, Clustering, Dimensionality Reduction, Unsupervised Learning},
  abstract={Feature selection is an important research area that seeks to eliminate unwanted features from
datasets. Many feature selection methods are suggested in the literature, but the evaluation of the best set of
features is usually performed using supervised metrics, where labels are required. In this work we propose a
methodology that tries to aid data specialists to answer simple but important questions, such as: (1) do current
feature selection methods give similar results? (2) is there is a consistently better method ? (3) how to select
the m-best features? (4) as the methods are not parameter-free, how to choose the best parameters in the
unsupervised scenario? and (5) given different options of selection, could we get better results if we fusion
the results of the methods? If yes, how can we combine the results? We analyze these issues and propose a
methodology that, based on some unsupervised methods, will make feature selection using strategies that turn
the execution of the process fully automatic and unsupervised, in high-dimensional datasets. After, we evaluate
the obtained results, when we see that they are better than those obtained by using the selection methods at
standard configurations. In the end, we also list some further improvements that can be made in future works.}
}
